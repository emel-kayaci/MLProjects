{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xd6IQ1ZczKO"
   },
   "source": [
    "# Design Choices in Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UolsLXSQczHh"
   },
   "source": [
    "###  Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "kwe0QooaczHk",
    "outputId": "02acba47-922f-4392-8592-d913060123f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from keras.models import Model\n",
    "import timeit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thEIzUfFczHy"
   },
   "source": [
    "### Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "-s6iKY5QczH0",
    "outputId": "1c24240d-370e-4212-ef09-dfeb7beab7e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 2\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# channels first: 1 (greyscale image) x 28 x 28\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    # number of examples, channels, rows, columns\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    # 28 x 28 x 1\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# normalize data     \n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weHnDsMoczKO"
   },
   "source": [
    "## Part 1: Influence of convolution size\n",
    "\n",
    "Try the models with different convolution sizes 5x5, 7x7 and 9x9 etc.\n",
    "\n",
    "Analyze the number of model parameters, accuracy and training time\n",
    "\n",
    "**Important changes:** To improve accuracy loss function is chosen binary_crossentropy instead of categorical_crossentropy. Adam optimizer is chosen instead of Adadelta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDmQdJkAczKP"
   },
   "source": [
    "### Model with (3 x 3) Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "id": "_LbhbPPwczKQ",
    "outputId": "40d19baf-55a5-45a3-86b2-170f3de0e69f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 16)        1168      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                294944    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 296,522\n",
      "Trainable params: 296,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.0423 - accuracy: 0.9858 - val_loss: 0.0163 - val_accuracy: 0.9945\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.0151 - accuracy: 0.9950 - val_loss: 0.0119 - val_accuracy: 0.9959\n",
      "Time Taken to run the model: 20.328688999999997 seconds\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "start = timeit.default_timer()   \n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "end = timeit.default_timer()\n",
    "print(\"Time Taken to run the model:\",end - start, \"seconds\") \n",
    "# 80 params: 3x3x1 (filter) x 8 (number of filters) + 8 (number of biases) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DnVhOxDczKS"
   },
   "source": [
    "### Try models with different Convolution sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oQopxYu5czKT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 8)         208       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 16)        3216      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                204832    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 208,586\n",
      "Trainable params: 208,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 14s 233us/step - loss: 0.0362 - accuracy: 0.9878 - val_loss: 0.0126 - val_accuracy: 0.9959\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 16s 258us/step - loss: 0.0118 - accuracy: 0.9960 - val_loss: 0.0090 - val_accuracy: 0.9969\n",
      "Time Taken to run the model: 29.854085399999995 seconds\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "start = timeit.default_timer()   \n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(16, (5, 5), activation='relu'))\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "end = timeit.default_timer()\n",
    "print(\"Time Taken to run the model:\",end - start, \"seconds\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oTAM1ojBn4aG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 22, 22, 8)         400       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 16)        6288      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                131104    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 138,122\n",
      "Trainable params: 138,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 16s 270us/step - loss: 0.0378 - accuracy: 0.9872 - val_loss: 0.0150 - val_accuracy: 0.9948\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 16s 266us/step - loss: 0.0130 - accuracy: 0.9957 - val_loss: 0.0094 - val_accuracy: 0.9968\n",
      "Time Taken to run the model: 32.5805564 seconds\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "start = timeit.default_timer()   \n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, kernel_size=(7, 7), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(16, (7, 7), activation='relu'))\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "end = timeit.default_timer()\n",
    "print(\"Time Taken to run the model:\",end - start, \"seconds\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PNOb8hDrn416"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 8)         656       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 16)        10384     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                73760     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 85,130\n",
      "Trainable params: 85,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 16s 264us/step - loss: 0.0420 - accuracy: 0.9858 - val_loss: 0.0168 - val_accuracy: 0.9943\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 16s 265us/step - loss: 0.0145 - accuracy: 0.9952 - val_loss: 0.0115 - val_accuracy: 0.9962\n",
      "Time Taken to run the model: 32.078411700000004 seconds\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "start = timeit.default_timer()   \n",
    "model = Sequential()\n",
    "# 4 filters\n",
    "model.add(Conv2D(8, kernel_size=(9, 9), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(16, (9, 9), activation='relu'))\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "end = timeit.default_timer()\n",
    "print(\"Time Taken to run the model:\",end - start, \"seconds\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONNy-NSdoB69"
   },
   "source": [
    "### Write your findings about activations here?\n",
    "\n",
    "1.   As the kernel increases in size, the number of parameters to compute also increases. This increases the processing time. It might be thought that when the kernel size increases the faster it will scan the whole picture, but the convolution is matrix multiplication and it is much faster than finding the appropriate parameter values.\n",
    "\n",
    "\n",
    "2. Accuracy is almost the same for each filter but may not be the same for different models. When I tried this CNN model for fashion mnist dataset, I observed that as the filter size increases, accuracy decreases. We can conclude from this that it may start loosing details in some smaller features where 3x3 or 5x5 would detect. So we need to increase kernel size if we want to detect larger features that cannot be detected by smaller kernel size and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbjj4YUAczKW"
   },
   "source": [
    "## Part 2: Influence of Striding\n",
    "\n",
    "Try the models with different stride sizes such as 2,3,4 etc.\n",
    "\n",
    "Analyze the number of model parameters, accuracy and training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pr45UR1uczKW"
   },
   "source": [
    "### Model with Convolution with 2 Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CUB3Zt_9czKX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 8)         80        \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 16)          1168      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                18464     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 20,042\n",
      "Trainable params: 20,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0754 - accuracy: 0.9743 - val_loss: 0.0348 - val_accuracy: 0.9887\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0290 - accuracy: 0.9905 - val_loss: 0.0209 - val_accuracy: 0.9932\n",
      "Time Taken to run the model: 5.833334199999996 seconds\n"
     ]
    }
   ],
   "source": [
    "# Stride specifies how many steps the filter takes\n",
    "start = timeit.default_timer()   \n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, kernel_size=(3, 3), strides=2, activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(16, (3, 3), strides=2, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "end = timeit.default_timer()\n",
    "print(\"Time Taken to run the model:\",end - start, \"seconds\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yG03EwvJovUK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 9, 9, 8)           80        \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 16)          1168      \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                4640      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 6,218\n",
      "Trainable params: 6,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1048 - accuracy: 0.9642 - val_loss: 0.0501 - val_accuracy: 0.9833\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0460 - accuracy: 0.9847 - val_loss: 0.0395 - val_accuracy: 0.9871\n",
      "Time Taken to run the model: 3.5518114000000054 seconds\n"
     ]
    }
   ],
   "source": [
    "# Stride specifies how many steps the filter takes\n",
    "start = timeit.default_timer()   \n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, kernel_size=(3, 3), strides=3, activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(16, (3, 3), strides=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "end = timeit.default_timer()\n",
    "print(\"Time Taken to run the model:\",end - start, \"seconds\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MaUGoQaPov0I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 8)           80        \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 2, 2, 16)          1168      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,658\n",
      "Trainable params: 3,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1639 - accuracy: 0.9407 - val_loss: 0.1024 - val_accuracy: 0.9625\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.0916 - accuracy: 0.9666 - val_loss: 0.0798 - val_accuracy: 0.9715\n",
      "Time Taken to run the model: 3.0297650999999917 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()   \n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, kernel_size=(3, 3), strides=4, activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(16, (3, 3), strides=4, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "end = timeit.default_timer()\n",
    "print(\"Time Taken to run the model:\",end - start, \"seconds\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nH_wTLUoDJH"
   },
   "source": [
    "### Write your findings about influence of striding here?\n",
    "\n",
    "1. The number of parameters decreases as it will scan the whole picture by taking bigger steps. The reason for this reduction is that the size of the image shrinks before entering the dense layer. This reduces processing time as it requires less computation.\n",
    "\n",
    "\n",
    "2. As the filter scans the picture by taking steps, it starts to miss some details, resulting in less accuracy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsIaUMRIczKb"
   },
   "source": [
    "## Part 3: Influence of Padding\n",
    "\n",
    "Try the models with padding and without padding.\n",
    "\n",
    "Analyze the number of model parameters, accuracy and training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8GP6BmVczKd"
   },
   "source": [
    "### Model with (3 x 3) Convolution with Same Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SZNcZ0BoczKe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 8)         80        \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 16)        1168      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                401440    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 403,018\n",
      "Trainable params: 403,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.0415 - accuracy: 0.9858 - val_loss: 0.0149 - val_accuracy: 0.9951\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.0141 - accuracy: 0.9953 - val_loss: 0.0115 - val_accuracy: 0.9963\n",
      "Time Taken to run the model: 27.386942800000014 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()   \n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, kernel_size=(3, 3), strides=1, padding='same', activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(16, (3, 3), strides=1, padding='same', activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "end = timeit.default_timer()\n",
    "print(\"Time Taken to run the model:\",end - start, \"seconds\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with (3 x 3) Convolution with Valid Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lKKnUSgJo0R2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 24, 24, 16)        1168      \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                294944    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 296,522\n",
      "Trainable params: 296,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0368 - accuracy: 0.9877 - val_loss: 0.0139 - val_accuracy: 0.9954\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.0128 - accuracy: 0.9958 - val_loss: 0.0113 - val_accuracy: 0.9961\n",
      "Time Taken to run the model: 21.733156000000008 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()   \n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, kernel_size=(3, 3), strides=1, padding='valid', activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(16, (3, 3), strides=1, padding='valid', activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "end = timeit.default_timer()\n",
    "print(\"Time Taken to run the model:\",end - start, \"seconds\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp-yZFx4oEqt"
   },
   "source": [
    "### Write your findings about influence of padding here?\n",
    "\n",
    "1. Without padding, the size of the image will gradually become smaller and the number of parameters will decrease accordingly. This speeds up the process.\n",
    "\n",
    "\n",
    "2. It is difficult to make a comment because the accuracy in these examples are very close. But when we have a much deeper model, if our padding type is only valid, the accuracy will start to decrease because the input size will constantly shrink and the detected features will be lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvGFyAkjczKj"
   },
   "source": [
    "## Part 4: Influence of Pooling\n",
    "\n",
    "Try the models with different pooling window sizes such as 2x2, 3x3, 4x4 etc.\n",
    "\n",
    "Analyze the number of model parameters, accuracy and training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1W63cBLczKk"
   },
   "source": [
    "### Model with (3 x 3) Convolution with Pooling (2 x 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-qOGWya1czKl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 11, 11, 16)        1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                12832     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 14,410\n",
      "Trainable params: 14,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.0686 - accuracy: 0.9760 - val_loss: 0.0252 - val_accuracy: 0.9913\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.0214 - accuracy: 0.9927 - val_loss: 0.0147 - val_accuracy: 0.9950\n",
      "Time Taken to run the model: 11.887521499999991 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()   \n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "end = timeit.default_timer()\n",
    "print(\"Time Taken to run the model:\",end - start, \"seconds\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUBHv14oczKp"
   },
   "source": [
    "### Model with (3 x 3) Convolution with Pooling (3 x 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "z3iYWjRDczKr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 6, 6, 16)          1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,658\n",
      "Trainable params: 3,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.1189 - accuracy: 0.9587 - val_loss: 0.0397 - val_accuracy: 0.9868\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 0.0367 - accuracy: 0.9876 - val_loss: 0.0269 - val_accuracy: 0.9906\n",
      "Time Taken to run the model: 9.29412289999999 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()   \n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "end = timeit.default_timer()\n",
    "print(\"Time Taken to run the model:\",end - start, \"seconds\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with (3 x 3) Convolution with Pooling (4 x 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4fsnp2wMpfTP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 26, 26, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 4, 4, 16)          1168      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 2,122\n",
      "Trainable params: 2,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1850 - accuracy: 0.9336 - val_loss: 0.0941 - val_accuracy: 0.9666\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0778 - accuracy: 0.9727 - val_loss: 0.0584 - val_accuracy: 0.9798\n",
      "Time Taken to run the model: 8.563369699999981 seconds\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()   \n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "end = timeit.default_timer()\n",
    "print(\"Time Taken to run the model:\",end - start, \"seconds\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCBC1uTjoGQB"
   },
   "source": [
    "### Write your findings about influence of pooling here?\n",
    "\n",
    "1. Pooling was an effect that greatly shortened the working time. The model which runs around 43 seconds without this effect, now achieves nearly the same accuracy in only 13 seconds. As the pooling size increases number of parameters decreases and it shortens the running time. When the pooling size increases it covers more area to find max values out of them.\n",
    "\n",
    "\n",
    "2. As the pooling size increases accuracy decreases. As the model starts to look for features in a wider area, it starts skipping some features that distinguish that picture from others."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Homework 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
